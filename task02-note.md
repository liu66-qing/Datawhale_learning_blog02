### 1.Sigmoid vs ReLU

**Sigmoid**：

- **公式**：σ(z)=11+e−z  \sigma(z) = \frac{1}{1 + e^{-z}}  σ(z)=1+e−z1
- **输出范围**：[0, 1]，是个平滑的S形曲线，从负无穷到正无穷逐渐从0过渡到1。
- **图像**：平滑过渡，中间斜率较大，两端趋于平坦（接近0或1）。
- **例子**：输入 z=b+wx  z = b + wx  z=b+wx，输出 c⋅σ(b+wx)  c \cdot \sigma(b + wx)  c⋅σ(b+wx)，通过调整 b  b  b、w  w  w、c  c  c 改变位置、斜率和高度。

**ReLU (Rectified Linear Unit)**：

- **公式**：ReLU(z)=max⁡(0,z)  \text{ReLU}(z) = \max(0, z)  ReLU(z)=max(0,z)
- **输出范围**：[0, 正无穷]，当 z≤0  z \leq 0  z≤0 输出0，当 z>0  z > 0  z>0 输出 z  z  z。
- **图像**：折线形状，z≤0  z \leq 0  z≤0 时水平，z>0  z > 0  z>0 时斜率为1。
- **例子**：输出 c⋅max⁡(0,b+wx)  c \cdot \max(0, b + wx)  c⋅max(0,b+wx)，两个ReLU可逼近一个Hard Sigmoid。



### 2.神经元

##### 神经元：

Sigmoid或ReLU就是一个神经元，多个神经元组成一层，层层叠加就是神经网络。

##### 隐藏层：

每层叫隐藏层，多层就是深度神经网络（深度学习）。

层数越深，模型越强，但过拟合风险也高。



##### 反向传播（BP）：高效算梯度，深度学习的核心技术。

### 3.机器学习框架

##### 数据

* 训练数据：$\{(\mathbf{x}^n, y^n)\}_{n=1}^N$，有特征和标签。
* 测试数据：$\{\mathbf{x}^{N+1}, \dots, \mathbf{x}^{N+M}\}$，只有特征

* 训练三步：
  * 定义模型：$f_\theta(\mathbf{x})$，$\theta$ 是所有参数。
  * 定义损失：$L(\theta)$，衡量预测误差。
    优化：找 $\theta^* = \arg\min_\theta L(\theta)$，用梯度下降。
  * 测试：用 $\theta^*$ 预测测试数据，提交结果（如Kaggle）。

### 4.超参数与注意事项

##### 超参数：

神经元数量：多能逼近复杂函数，但计算量大，易过拟合。
批量大小：影响训练速度和稳定性。
隐藏层层数：层多模型强，但过深可能过拟合。

过拟合：训练数据表现好，测试数据差。解决办法：选测试表现好的模型（如3层优于4层）。
应用例子：预测观看次数（用前56天数据）。模型能抓周期性低谷，但像除夕这种特殊日子，单靠历史数据可能预测不准。
